# Aurora

Aurora is a mobile application built using Flutter that aims to provide navigation assistance and object detection for visually impaired individuals. The app leverages the power of TensorFlow, Google ML Kit, and Firebase to deliver an accessible and user-friendly experience. We have implemented a minimal user interface for everyone to try our app for a better cause and a better tomorrow.

<img src="https://img.freepik.com/free-photo/metaverse-avatar-collage-concept_52683-96429.jpg?w=996&t=st=1710648001~exp=1710648601~hmac=f6ee6a3fe2a4bb9a7f31a829555a6efee5dcecb45d3a488af74d29d824212223" alt="Banner">




## Features

- **Object Detection**: Utilizing Google ML Kit's object detection capabilities, the app can identify and describe various objects in the user's surroundings, providing crucial information about their environment.
- **Navigation Assistance**: With the integration of Firebase, Aurora offers navigation instructions, helping users navigate through indoor spaces.
- **Voice Commands**: The app supports voice feedback, allowing users to interact with the application.
- **Accessible Design**: The user interface is designed with accessibility in mind, ensuring a seamless experience for visually impaired users.

## Tech Stack

- **Flutter**: The app is built using the Flutter framework, enabling cross-platform development for both iOS and Android devices.
- **TensorFlow Lite**: TensorFlow Lite is used for on-device machine learning models, enabling real-time object detection and classification.
- **Google ML Kit**: Google ML Kit provides pre-trained machine learning models for object detection and other computer vision tasks.
- **Firebase**: Firebase is utilized for backend services such as real-time database, authentication, and cloud functions for navigation assistance.

## Future Scopes
- Integration with Wearable Devices: As wearable technology advances, integrating Aurora with devices like smart glasses or wearable sensors can offer even greater convenience and independence to visually impaired users. These devices can provide real-time feedback on their surroundings without needing to hold a smartphone.
- Multi-Language Support: Expanding language support beyond English can make the app accessible to a broader audience globally. This can involve translating both the user interface and the object detection descriptions into multiple languages.

## Business Model
- Freemium Model: Offer a basic version of the app for free, with limited features such as basic object detection and navigation assistance. Premium features like advanced object detection, offline navigation, and personalized assistance can be unlocked through a subscription model.
- Corporate Partnerships: Forge partnerships with companies and organizations that cater to visually impaired individuals, such as blind associations, rehabilitation centers, or assistive technology providers. Offer customized versions of the app tailored to their specific needs and provide training and support services.

## Landing Page
- We have designed a landing page for our Application to to give everyone a quick glance of what we have made and what we are doing to do to create a better tomorrow
- Website link -> https://devshouse-aurora.vercel.app

## Setup

To run the Aurora app locally, follow these steps:

1. Clone the repository:
2. Navigate to the cloned directory:
3. Run: `flutter pub get`
4. Run: `flutter run`
 
